{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Many_to_many.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piecesofreg09/Eng_to_Italian/blob/master/Many_to_many.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCVJBXUMGSX0",
        "colab_type": "text"
      },
      "source": [
        "Char level\n",
        "\n",
        "Word level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP6SCdgllKof",
        "colab_type": "code",
        "outputId": "faf01283-c10f-4126-ba34-a937f31ee5b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdMErsqSlMF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = 'gdrive/My Drive/Colab Notebooks/Models/Many2many/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGNzaLbUIl4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import random\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import load_model\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78YDQuFzFScV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/piecesofreg09/Eng_to_Italian/master/Data/ita.txt'\n",
        "#url = 'https://raw.githubusercontent.com/piecesofreg09/Eng_to_Italian/master/Data/fra.txt'\n",
        "df0 = pd.read_csv(url, sep='\\t', header=None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPeYGSx6IpWB",
        "colab_type": "code",
        "outputId": "caf4d0b0-f1bf-4a86-a44e-f856e93db020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df0.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Ciao!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Corri!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Corra!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Correte!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Chi?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0         1\n",
              "0   Hi.     Ciao!\n",
              "1  Run!    Corri!\n",
              "2  Run!    Corra!\n",
              "3  Run!  Correte!\n",
              "4  Who?      Chi?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12r2VP2xWgiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras while running the program'\n",
        "    def __init__(self, total_rows, vocab_paras, data_en, data_it,\n",
        "                 batch_size=1024, shuffle=True):\n",
        "        '''\n",
        "        Initialization\n",
        "        vocab_paras: the max length of sentences in en and it, \n",
        "                     the number of characters in en and it\n",
        "        data_en, data_it: the input data, english data and italian data are separate\n",
        "        '''\n",
        "        self.total_rows = total_rows\n",
        "        self.sent_max_en, self.vocab_size_en, \\\n",
        "            self.sent_max_it, self.vocab_size_it = vocab_paras\n",
        "        \n",
        "        self.data_en = data_en\n",
        "        self.data_it = data_it\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        \n",
        "        self.indices = np.arange(self.total_rows)\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(self.total_rows / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        temp_index = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(temp_index)\n",
        "        \n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indices = np.arange(self.total_rows)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, temp_index):\n",
        "        'Generates data containing batch_size samples' \n",
        "        # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X_1 = np.zeros((self.batch_size, self.sent_max_en, self.vocab_size_en), \n",
        "                       dtype='float32')\n",
        "        X_2 = np.zeros((self.batch_size, self.sent_max_it, self.vocab_size_it),\n",
        "                       dtype='float32')\n",
        "        y_2 = np.zeros((self.batch_size, self.sent_max_it, self.vocab_size_it), \n",
        "                       dtype='float32')\n",
        "        \n",
        "        \n",
        "        # Generate data\n",
        "        for i, idd in enumerate(temp_index):\n",
        "            for j, charr in enumerate(self.data_en[idd]):\n",
        "                X_1[i, j, char_to_ix_en[charr]] = 1.0\n",
        "            for j, charr in enumerate(self.data_it[idd]):\n",
        "                X_2[i, j, char_to_ix_it[charr]] = 1.0\n",
        "                if j > 0:\n",
        "                    y_2[i, j - 1, char_to_ix_it[charr]] = 1.0\n",
        "        \n",
        "        #print(self.data_en[temp_index[1]])\n",
        "        #print(self.data_it[temp_index[1]])\n",
        "        \n",
        "        return [X_1, X_2], y_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiD6l7jeWOdG",
        "colab_type": "code",
        "outputId": "8747564d-a49d-481f-b835-6109d013537c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df1 = df0.sample(n=200000)\n",
        "#df1 = df0.loc[:100000]\n",
        "df1.reset_index(inplace=True, drop=True)\n",
        "df1.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I didn't want to go, but I needed to.</td>\n",
              "      <td>Io non volevo andare, però dovevo.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tom had his wallet stolen while he was in Boston.</td>\n",
              "      <td>A Tom hanno rubato il portafoglio mentre era a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tom is not good enough.</td>\n",
              "      <td>Tom non è abbastanza bravo.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Take my hand.</td>\n",
              "      <td>Prendete la mia mano.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>He struck at me with a stick.</td>\n",
              "      <td>Mi colpì con un bastone.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                                                  1\n",
              "0              I didn't want to go, but I needed to.                 Io non volevo andare, però dovevo.\n",
              "1  Tom had his wallet stolen while he was in Boston.  A Tom hanno rubato il portafoglio mentre era a...\n",
              "2                            Tom is not good enough.                        Tom non è abbastanza bravo.\n",
              "3                                      Take my hand.                              Prendete la mia mano.\n",
              "4                      He struck at me with a stick.                           Mi colpì con un bastone."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCnxZW-gI0_Q",
        "colab_type": "code",
        "outputId": "44a44108-95f7-4682-c3ee-de5a74c233db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data_en = list(map(lambda x: x, list(df1[0])))\n",
        "chunks_en = ''.join(list(map(str, data_en)))\n",
        "sent_max_en = np.max([len(s) for s in data_en])\n",
        "chars_en = list(set(chunks_en))\n",
        "data_size_en, vocab_size_en = len(data_en), len(chars_en)\n",
        "print('There are %d total sentences and %d unique characters in English.'\n",
        "      % (data_size_en, vocab_size_en))\n",
        "print('Max sentence length is %d ' % sent_max_en)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 200000 total sentences and 87 unique characters in English.\n",
            "Max sentence length is 262 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SihbFw-rM1a_",
        "colab_type": "code",
        "outputId": "27264eeb-5c37-4d9a-e3aa-d98995d9aa83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "data_it = list(map(lambda x: '\\t' + x + '\\n', list(df1[1])))\n",
        "chunks_it = ''.join(list(map(str, data_it)))\n",
        "sent_max_it = np.max([len(s) for s in data_it])\n",
        "chars_it = list(set(chunks_it))\n",
        "data_size_it, vocab_size_it = len(data_it), len(chars_it)\n",
        "print('There are %d total sentences and %d unique characters in Italian.'\n",
        "      % (data_size_it, vocab_size_it))\n",
        "print('Max sentence length is %d ' % sent_max_it)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 200000 total sentences and 95 unique characters in Italian.\n",
            "Max sentence length is 305 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuuWeIW2ZldA",
        "colab_type": "code",
        "outputId": "0eaadc86-6abc-4773-e4ca-5d7739b950ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sp = 100\n",
        "data_en[sp] + data_it[sp]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tom just got arrested.\\tTom è appena stato arrestato.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEURSd_6NzAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_to_ix_en = { ch:i for i,ch in enumerate(sorted(chars_en)) }\n",
        "ix_to_char_en = { i:ch for i,ch in enumerate(sorted(chars_en)) }\n",
        "char_to_ix_it = { ch:i for i,ch in enumerate(sorted(chars_it)) }\n",
        "ix_to_char_it = { i:ch for i,ch in enumerate(sorted(chars_it)) }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Xjm9zlP2B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "latent_dim = 256\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, vocab_size_en))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, vocab_size_it))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size_it, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE6cUFBdh0FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_train, en_valid, it_train, it_valid = train_test_split(\n",
        "        data_en, data_it, test_size=0.10, random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0W_guVih5Q_",
        "colab_type": "code",
        "outputId": "25eca61b-e9cd-410c-f08e-622cb7ae68cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "np.array(list(zip(en_train[0:10:2] ,it_train[0:10:2])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[\"I wasn't asking for your opinion.\",\n",
              "        '\\tNon stavo chiedendo la tua opinione.\\n'],\n",
              "       ['He likes chicken nuggets.',\n",
              "        '\\tGli piacciono le crocchette di pollo.\\n'],\n",
              "       ['This box is so large that it cannot go into my bag.',\n",
              "        '\\tQuesta scatola è così grande che non riesce ad entrare nella mia borsa.\\n'],\n",
              "       [\"Maybe it's best not to add pepper.\",\n",
              "        '\\tForse è meglio non aggiungere pepe.\\n'],\n",
              "       [\"You're so paranoid.\", '\\tÈ così paranoica.\\n']], dtype='<U73')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgcl92nPswXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot figure lof losses\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        print(logs)\n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAlfXw7Pqato",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_epoch(path):\n",
        "    x = path.find('weights')\n",
        "    print(path)\n",
        "    return int(path[x+8:x+11])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s6usCObK7Rs",
        "colab_type": "code",
        "outputId": "2505cba5-4a1d-42da-d15d-9c7d72c8fb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vocab_paras = [sent_max_en, vocab_size_en, \n",
        "               sent_max_it, vocab_size_it]\n",
        "training_generator = DataGenerator(len(en_train), vocab_paras,\n",
        "                                   en_train, it_train, \n",
        "                                   batch_size=512,shuffle=True)\n",
        "validation_generator = DataGenerator(len(en_valid), vocab_paras,\n",
        "                                     en_valid, it_valid, \n",
        "                                     batch_size=64, shuffle=True)\n",
        "# training\n",
        "check_path = 'gdrive/My Drive/Colab Notebooks/Models/Many2many/checkpoint/'\n",
        "checkpoint = ModelCheckpoint(check_path + 'weights.{epoch:03d}-{val_loss:.6f}.hdf5',\n",
        "                            monitor='val_loss')\n",
        "\n",
        "if len(os.listdir(check_path)) != 0:\n",
        "    list_of_files = glob.glob(check_path + '*') # * means all if need specific format then *.csv\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    # Load model:\n",
        "    model = load_model(latest_file)\n",
        "    # Finding the epoch index from which we are resuming\n",
        "    initial_epoch = get_init_epoch(latest_file)\n",
        "else:\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    initial_epoch = 0\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit_generator(generator=training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    use_multiprocessing=True,\n",
        "                    callbacks=[plot_losses, checkpoint],\n",
        "                    epochs=100, \n",
        "                    initial_epoch=initial_epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDX2llPIi/x9pQexWxEC7tAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13/100\n",
            " 42/351 [==>...........................] - ETA: 7:20 - loss: 0.0916"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-262:\n",
            "Process ForkPoolWorker-263:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-190-8ac419107e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplot_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     initial_epoch=initial_epoch)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE66d51FvZmS",
        "colab_type": "text"
      },
      "source": [
        "It can be seen from the above figure, validation loss is not bad. However if we sample the sentences it's not good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-cHjiYp8akx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(root_path + 's2s.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIjqf_XQ6F5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder model is separated from the complex model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "\n",
        "# decoder model is also just LSTM + Dense\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NKM2L2279jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, vocab_size_it))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, char_to_ix_it['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = ix_to_char_it[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > sent_max_it):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, vocab_size_it))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFMcliawgb5S",
        "colab_type": "code",
        "outputId": "321580cf-2c98-45ad-f8fa-9a6fe1ebaa84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "indices_test = np.arange(df1.shape[0])\n",
        "indices_test_30 = indices_test[10:300:10]\n",
        "\n",
        "X_1_test = np.zeros((len(indices_test_30), sent_max_en, vocab_size_en))\n",
        "\n",
        "# Using training data to check the result\n",
        "for i, idd in enumerate(indices_test_30):\n",
        "    for j, charr in enumerate(data_en[idd]):\n",
        "        X_1_test[i, j, char_to_ix_en[charr]] = 1\n",
        "    \n",
        "for seq_index in range(10):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = X_1_test[seq_index: seq_index + 1]\n",
        "    print(input_seq.shape)\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence: ', data_en[seq_index])\n",
        "    print('What it should be: ', data_it[seq_index][:-1])\n",
        "    print('Decoded sentence: ', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  I didn't want to go, but I needed to.\n",
            "What it should be:  \tIo non volevo andare, però dovevo.\n",
            "Decoded sentence:  Si è andato a casa.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  Tom had his wallet stolen while he was in Boston.\n",
            "What it should be:  \tA Tom hanno rubato il portafoglio mentre era a Boston.\n",
            "Decoded sentence:  Tom ha comprato un conto di Mary.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  Tom is not good enough.\n",
            "What it should be:  \tTom non è abbastanza bravo.\n",
            "Decoded sentence:  Tom è in casa.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  Take my hand.\n",
            "What it should be:  \tPrendete la mia mano.\n",
            "Decoded sentence:  Tom è un po' stanco.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  He struck at me with a stick.\n",
            "What it should be:  \tMi colpì con un bastone.\n",
            "Decoded sentence:  Non sono stato a Boston.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  You look surprised.\n",
            "What it should be:  \tSembrate sorprese.\n",
            "Decoded sentence:  Tom ha detto che non lo farà da sola.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  Tom buttered his toast.\n",
            "What it should be:  \tTom ha imburrato il suo toast.\n",
            "Decoded sentence:  Io ho un po' di tempo a casa.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  I respect everybody's opinion.\n",
            "What it should be:  \tIo rispetto l'opinione di tutti.\n",
            "Decoded sentence:  Io ho comprato un contro di Tom e Mary solamente un po' di tempo per tutto il giorno di cosa.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  Let's talk about your career.\n",
            "What it should be:  \tParliamo della sua carriera.\n",
            "Decoded sentence:  Tom non è ancora arrabbiato con Mary.\n",
            "\n",
            "(1, 262, 87)\n",
            "-\n",
            "Input sentence:  Shouldn't you be at home with your mother?\n",
            "What it should be:  \tNon dovresti essere a casa con tua madre?\n",
            "Decoded sentence:  Tom ha comprato un conto di Mary.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyOTP-rH_j75",
        "colab_type": "code",
        "outputId": "8ca29cf0-2114-475e-be28-3ca54a47b172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "indices_test = np.arange(df0.shape[0])\n",
        "indices_test_30 = indices_test[100:300:10]\n",
        "\n",
        "X_1_test = np.zeros((len(indices_test_30), sent_max_en, vocab_size_en))\n",
        "\n",
        "\n",
        "data_en_test = list(map(lambda x: x.lower(), list(df0[0])))\n",
        "data_it_test = list(map(lambda x: x.lower(), list(df0[1])))\n",
        "# Generate\n",
        "for i, idd in enumerate(indices_test_30):\n",
        "    for j, charr in enumerate(data_en_test[idd]):\n",
        "        X_1_test[i, j, char_to_ix_en[charr]] = 1\n",
        "    \n",
        "for seq_index in range(10):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = X_1_test[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence: ', data_en[seq_index])\n",
        "    print('What it should be: ', data_it[seq_index][:-1])\n",
        "    print('Decoded sentence: ', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence:  I didn't want to go, but I needed to.\n",
            "What it should be:  \tIo non volevo andare, però dovevo.\n",
            "Decoded sentence:  La sua casa è adesso.\n",
            "\n",
            "-\n",
            "Input sentence:  Tom had his wallet stolen while he was in Boston.\n",
            "What it should be:  \tA Tom hanno rubato il portafoglio mentre era a Boston.\n",
            "Decoded sentence:  La vostra scuola.\n",
            "\n",
            "-\n",
            "Input sentence:  Tom is not good enough.\n",
            "What it should be:  \tTom non è abbastanza bravo.\n",
            "Decoded sentence:  La vostra scuola.\n",
            "\n",
            "-\n",
            "Input sentence:  Take my hand.\n",
            "What it should be:  \tPrendete la mia mano.\n",
            "Decoded sentence:  Come sono presto?\n",
            "\n",
            "-\n",
            "Input sentence:  He struck at me with a stick.\n",
            "What it should be:  \tMi colpì con un bastone.\n",
            "Decoded sentence:  Come sembra interessante.\n",
            "\n",
            "-\n",
            "Input sentence:  You look surprised.\n",
            "What it should be:  \tSembrate sorprese.\n",
            "Decoded sentence:  Segui a casa.\n",
            "\n",
            "-\n",
            "Input sentence:  Tom buttered his toast.\n",
            "What it should be:  \tTom ha imburrato il suo toast.\n",
            "Decoded sentence:  La sua casa è adesso.\n",
            "\n",
            "-\n",
            "Input sentence:  I respect everybody's opinion.\n",
            "What it should be:  \tIo rispetto l'opinione di tutti.\n",
            "Decoded sentence:  Io vorrei continuare.\n",
            "\n",
            "-\n",
            "Input sentence:  Let's talk about your career.\n",
            "What it should be:  \tParliamo della sua carriera.\n",
            "Decoded sentence:  Io vorrei continuare.\n",
            "\n",
            "-\n",
            "Input sentence:  Shouldn't you be at home with your mother?\n",
            "What it should be:  \tNon dovresti essere a casa con tua madre?\n",
            "Decoded sentence:  La sua casa è adesso.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NW5l-IU_lJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}